# GSP 311 (Automate Interactions with Contact Center AI: Challenge Lab)

<!-- Disclaimer Section -->
> **Warning**
> <br>The solutions shown here might not work if there are task changes in the future.

![last_updated_19062022](https://img.shields.io/badge/last%20updated-19%20June%202022-red)

<!-- Lab Description With Link -->
Configure and deploy the Speech Analysis Framework and be able to leverage BigQuery for insights on data extracted from call recordings on a call center environment.

Link to the challenge lab: [click here](https://www.cloudskillsboost.google/focuses/12008?parent=catalog)

## Challenge Scenario 

Your company is ready to launch a new marketing campaign. For that, they would like to know what customers have been calling customer service about and what is the sentiment around those topics. 

They will be sending the data to a third-party for further analysis, so sensitive data such as customer's name, phone number, address, email, SSN, should all be redacted from the data. Right now all the data that they have is available as call recordings and they have not been processed/analyzed yet.

---

<!-- Task and Solution -->
# Task 1: Create A Storage Bucket

Run the following code in Cloud Shell:

```bash
export PROJECT=$(gcloud info --format='value(config.project)')
git clone https://github.com/GoogleCloudPlatform/dataflow-contact-center-speech-analysis.git
```

```bash
gsutil mb -l <REGION_NAME> gs://<BUCKET_NAME>/
```

> **Note** 
> <br>Remember to replace
> <br>`<REGION_NAME>` with the "Region Name" given in the task at the left panel.
> <br>`<BUCKET_NAME>` with the "Bucket Name" given in the task at the left panel.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 2: Create A Cloud Function

Run the following code in Cloud Shell:

```bash
cd dataflow-contact-center-speech-analysis/saf-longrun-job-func/
```

```bash
gcloud functions deploy safLongRunJobFunc --runtime nodejs12 --trigger-resource <TASK1_BUCKET_NAME> --region <REGION_NAME> --trigger-event google.storage.object.finalize
```

> **Note** 
> <br>Remember to replace
> <br>`<REGION_NAME>` with the "Region Name" given in the task at the left panel.
> <br>`<TASK1_BUCKET_NAME>` with the "Bucket Name" given in the task at the left panel.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 3: Create A BigQuery Dataset

Run the following code in Cloud Shell:

```bash
bq mk <BQ_DATASET_NAME>
```

> **Note** 
> <br>Remember to replace `<BQ_DATASET_NAME>` with the "BigQuery Dataset Name" given in the task at the left panel.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 4: Create A Pub/Sub Topic

Run the following code in Cloud Shell:

```bash
gcloud pubsub topics create <PUB/SUB_TOPIC_NAME>
```

> **Note** 
> <br>Remember to replace `<PUB/SUB_TOPIC_NAME>` with the "Pub/Sub Topic Name" given in the task at the left panel.


```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 5: Create A Cloud Storage Bucket For Staging Contents

1. Type "Bucket" in the search bar to enter Cloud Storage.
2. Click **CREATE BUCKET** and enter the `<TASK5_BUCKET_NAME>` at name > **CREATE**.
3. Click on the created bucket `<TASK5_BUCKET_NAME>` > **CREATE FOLDER**.
4. Type the `<DATAFLOW_OBJECT_NAME>` into the field > **CREATE**.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 6: Deploy A Cloud Dataflow Pipeline

Run the following code in Cloud Shell:

```bash
cd dataflow-contact-center-speech-analysis/saf-longrun-job-dataflow
```

then this:

```bash
python -m virtualenv env -p python3
source env/bin/activate
pip install apache-beam[gcp]
pip install dateparser

export PROJECT_ID=<PROJECT_ID>
export TOPIC_NAME=<TOPIC_NAME>
export BUCKET_NAME=<TASK5_BUCKETNAME>
export DATASET_NAME=<DATASET_NAME>
export TABLE_NAME=transcript

python3 saflongrunjobdataflow.py --project=$PROJECT_ID \
--input_topic=projects/$PROJECT_ID/topics/$TOPIC_NAME \
--runner=DataflowRunner \
--region=<REGION_NAME> \
--temp_location=gs://$BUCKET_NAME/<DATAFLOW_OBJECT_NAME> \
--output_bigquery=${PROJECT}:$DATASET_NAME.transcripts \
--requirements_file="requirements.txt"
```

> **Note** 
> <br>Remember to replace
> <br>`<PROJECT_ID>` with the "Project ID" given in the task at the left panel.
> <br>`<TOPIC_NAME>` with the "Topic Name" given in the task at the left panel.
> <br>`<TASK5_BUCKET_NAME>` with the "Bucket Name" given in task 5.
> <br>`<DATASET_NAME>` with the "Dataset Name" given in the task at the left panel.
> <br>`<DATAFLOW_OBJECT_NAME>` with the "Dataflow Object Name" given in the task at the left panel.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 7: Upload Sample Audio Files For Preprocessing

Run the following code in Cloud Shell:

```bash
    # mono flac audio sample
gsutil -h x-goog-meta-dlp:false -h x-goog-meta-callid:1234567 -h x-goog-meta-stereo:false -h x-goog-meta-pubsubtopicname:$TOPIC_NAME -h x-goog-meta-year:2019 -h x-goog-meta-month:11 -h x-goog-meta-day:06 -h x-goog-meta-starttime:1116 cp gs://qwiklabs-bucket-gsp311/speech_commercial_mono.flac gs://<TASK1_BUCKET_NAME>
```

> **Note** 
> <br>Remember to replace `<TASK1_BUCKET_NAME>` with the "Bucket Name" given in the task at the left panel.

```bash
    # stereo wav audio sample
gsutil -h x-goog-meta-dlp:false -h x-goog-meta-callid:1234567 -h x-goog-meta-stereo:true -h x-goog-meta-pubsubtopicname:$TOPIC_NAME -h x-goog-meta-year:2019 -h x-goog-meta-month:11 -h x-goog-meta-day:06 -h x-goog-meta-starttime:1116 cp gs://qwiklabs-bucket-gsp311/speech_commercial_stereo.wav gs://<TASK1_BUCKET_NAME>
```

> **Note** 
> <br>Remember to replace `<TASK1_BUCKET_NAME>` with the "Bucket Name" given in the task at the left panel.

Open BigQuery and wait for ~5 minutes until the table "transcripts" is created, then proceed to the next task.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 8: Run A Data Loss 

1. Open a new query editor and enter the following:

    ```sql
    CREATE OR REPLACE TABLE `<PROJECT_ID>.<DATASET_NAME>.copy` AS
    SELECT * FROM `<PROJECT_ID>.<DATASET_NAME>.transcripts`
    ```

    > **Note** 
    > <br>Remember to replace
    > <br>`<PROJECT_ID>` with the "Project ID" given in the task at the left panel.
    > <br>`<DATASET_NAME>` with the "Dataset Name" given in the task at the left panel.

2. Click the "copy" table, then click **EXPORT** > **SCAN WITH DLP**.
3. Enter a job id that u like > **CREATE** > **COFIRM CREATE**, then wait until the scanning is completed.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

---

<!-- Completion Section -->
# Completion

At this point, you should have completed the lab.