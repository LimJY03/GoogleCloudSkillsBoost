# GSP323 (Perform Foundational Data, ML, and AI Tasks in Google Cloud: Challenge Lab)

<!-- Disclaimer Section -->
> **Warning**
> <br>The solutions shown here might not work if there are task changes in the future.

![last_updated_23072022](https://img.shields.io/badge/last%20updated-23%20July%202023-yellow)

<!-- Lab Description With Link -->
This challenge lab tests your skills and knowledge from the labs in the Baseline Data, ML and AI, quest. You should be familiar with the content of the labs before attempting this lab.

Link to the challenge lab: [click here](https://www.cloudskillsboost.google/focuses/11044?parent=catalog)

## Challenge Scenario 

As a junior data engineer in Jooli Inc. and recently trained with Google Cloud and a number of data services you have been asked to demonstrate your newly learned skills. The team has asked you to complete the following tasks.

---

<!-- Task and Solution -->
# Task 1: Run A Simple Dataflow Job

1. Ensure that the necessary Dataflow API is enabled, restart the connection to the Dataflow API by following the steps below:
    * In the Cloud Console, enter "Dataflow API" in the top search bar. Click on the result for **Dataflow API**.
    * Click **Manage** > **Disable API** > **Disable**.
    * After the API is disabled, click **Enable**.
2. Open Cloud Shell and execute to create a BigQuery data set, replacing `<BIG_QUERY_DATASET_NAME>` with the name given in the task ("Create a BigQuery dataset called _").

    ```bash
    bq mk <BIG_QUERY_DATASET_NAME>
    ```

3. Then, execute the following code to obtain the schema of the data set.

    ```bash
    gsutil cp gs://cloud-training/gsp323/lab.schema .
    ```

    ```bash
    cat lab.schema
    ```

    This will open up the schema for the data set. We will need to copy it later on.
4. To create BigQuery data set, type "BigQuery" in the search bar to enter BigQuery. Then, click on the project ID at the left and select the created data set.
5. Click **CREATE TABLE** and enter each field with the following:
    * Under **Source**, set each of the field to:
        * **Create table from**: `Google Cloud Storage`
        * **Select file from GCS bucket**: `gs://cloud-training/gsp323/lab.csv`

            > **Note**
            > <br>If error occurs, then paste the path after '`gs://`' only.

        * **File format**: `CSV`
    * Under **Destination**, set each of the field to:
        * Set the **Table Name** to the one behind the final `.` given in the task (BigQuery Output Table). For example: `customer_164`
    * Under **Schema**, toggle on **Edit as text**, and paste the schema from Cloud Shell to the field below. It should look like this:

        ```json
        [
            {"type":"STRING","name":"guid"},
            {"type":"BOOLEAN","name":"isActive"},
            {"type":"STRING","name":"firstname"},
            {"type":"STRING","name":"surname"},
            {"type":"STRING","name":"company"},
            {"type":"STRING","name":"email"},
            {"type":"STRING","name":"phone"},
            {"type":"STRING","name":"address"},
            {"type":"STRING","name":"about"},
            {"type":"TIMESTAMP","name":"registered"},
            {"type":"FLOAT","name":"latitude"},
            {"type":"FLOAT","name":"longitude"}
        ]
        ```

    Then, click **Create Table**.
6. To create a bucket, type "Storage" to enter Cloud Storage. Then, click **Create a Storage Bucket** and enter the **Name** as provided in the task ("Create a Cloud Storage Bucket called _").
7. Next, type "Dataflow Jobs" at the search bar to enter the Job tab of Dataflow. Then, click **Create Job from Template**.
8. Then for **Dataflow Template**, choose **Text Files on Cloud Storage to BigQuery** under "Process Data in Bulk (batch)", leave **Regional Endpoint** as default. At "Job name", type any name as you want.

    Under **Required Parameters**, follow the steps below.

    > **Note**
    > <br>Make sure that the chosen "Text Files on Cloud Storage to BigQuery" is under "Process Data in Bulk (batch)", not "Process Data Continuously (Stream)".

    > **Note**
    > <br>For the following URLs, ignore '`gs://`' if it is provided in the field.already.

    * At "JavaScript UDF ... Storage", type `gs://cloud-training/gsp323/lab.js`.
    * At "JSON path", type `gs://cloud-training/gsp323/lab.schema`.
    * At "JavaScript UDF name", type `transform`.
    * At "BigQuery output table", type the name given in the task (BigQuery Output Table).
    * At "Cloud storage input path", type `gs://cloud-training/gsp323/lab.csv`
    * At "Temporary directory ... process", paste the directory given in the task (Temporary BigQuery directory).
    * At "Temporary location", paste the directory given in the task (Temporary location).
    * Click Optional Parameters and uncheck "Use default machine type", then select **E2** series and **e2-standard-2** machine type.

    Then, click **Run Job**. After the job has completed running, proceed to the next step.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

---

# Task 2: Run A Simple Dataproc Job

1. Type "Dataproc" in the search bar to enter Dataproc. In **Clusters** tab, click **Create Cluster** > **Create Cluster on Compute Engine** at the middle of the page. 
    * For "Region", enter the region given in the task (Region).
    * Under **Configure nodes (optional)**, change the "Series" to **E2** and the "Machine type" to **e2-standard-2** for both "Manager node" and "Worker nodes"
    
    Leave the others at default and click **Create**.
2. Click into the created cluster, then click **VM Instances** tab. Then, click the **SSH** button under the "-m" suffix (Master role) instance.
3. On the pop-up SSH window, enter the following code.

    ```bash
    hdfs dfs -cp gs://cloud-training/gsp323/data.txt /data.txt
    ```

    > **Note**
    > <br>This must be done in the SSH pop-up window, not in Cloud Shell.

4. Exit the SSH and click **Submit Job** near the top of the page. Enter each field with the following.
    * At "Job ID", leave it.
    * At "Job Type", choose **Spark**.
    * At "Main class or jar", type `org.apache.spark.examples.SparkPageRank`.
    * At "Jar files", type `file:///usr/lib/spark/examples/jars/spark-examples.jar`.
    * At "Archive files", leave it.
    * At "Arguments", type `/data.txt`.
    * At "Max restarts per hour", type `1`.

    Then, click **Submit**. After the job has completed running, proceed to the next step.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 3: Use the Google Cloud Speech API

1. Type "Credentials" in the search bar to enter the Credentials page under APIs & Services. Click the `+ CREATE CREDENTIALS` > "API Key" to create your API key. Save the API key in your notepad.
2. Go back to the dataproc cluster by typing "Dataproc" in the search bar, click into the cluster you have created and click **VM Instances** tab.
3. Click the **SSH** button under the "-m" suffix (Master role) instance again.
4. On the pop-up SSH window, enter the following code.

   ```bash
   export API_KEY=<YOUR_COPIED_API_KEY>
   ```

5. Then, we will create a `request.json` file by entering the following code.

   ```bash
   touch request.json
   nano request.json
   ```

6. This will open up an editor, paste the following code into the editor.

   ```json
   {
        "config": {
            "encoding": "FLAC",
            "languageCode": "en-US"
        },
        "audio": {
            "uri": "gs://cloud-training/gsp323/task3.flac"
        }
   }
   ```

7. Close the editor by clicking the keys "CTRL+X" > "Y" > "Enter" in sequence.

   > **Note**
   > <br>Remember to replace the `<YOUR_COPIED_API_KEY>` with the API Key you copied just now.

8. Then, type in the following to call the Google Cloud Speech API.

   ```bash
   curl -s -X POST -H "Content-Type: application/json" --data-binary @request.json "https://speech.googleapis.com/v1/speech:recognize?key=${API_KEY}" > result.json

   cat result.json
   ```

   > **Note**
   > <br>If the output of `result.json` contains "Invalid API Key", run the `export API_KEY=...` command at step 4 again and re-run the `curl -s ...` command in this step again.

9. Finally, we will copy the `result.json` file into the Upload Path given in the lab task by entering the code below.

   ```bash
   gsutil cp result.json <UPLOAD_PATH>
   ```

   > **Note**
   > <br>Remember to replace the `<UPLOAD_PATH>` with the path given in the lab task, example: `gsutil cp result.json gs://qwiklabs-gcp-04-7dc18edf6c77-marking/task3-gcs-440.result `

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 4: Use the Cloud Natural Language API

For this section, choose one of the AI tasks given in the challenge lab task. The code below is for the question on **Cloud Natural Language API**.

1. In Cloud Shell, run the following code one-by-one.

    ```bash
    gcloud iam service-accounts create my-natlang-sa \
      --display-name "my natural language service account"

    gcloud iam service-accounts keys create ~/key.json \
      --iam-account my-natlang-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com

    export GOOGLE_APPLICATION_CREDENTIALS="/home/$USER/key.json"

    gcloud auth activate-service-account my-natlang-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --key-file=$GOOGLE_APPLICATION_CREDENTIALS

    gcloud ml language analyze-entities --content="Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat." > result.json
    ```

3. Execute the following part, type 'y' and a link will be generated.

    ```bash
    gcloud auth login
    ```

    > **Warning**
    > <br>If your device does not have web browser, run the following code instead.
    > 
    > ```bash
    > gcloud auth login --no-launch-browser
    > ```

    Click the link and authorize everything, copy the token generated, then return to the Cloud Shell and paste the token at the prompt.

4. Replace the `<UPLOAD_PATH>` with the url given in the task (Use the Cloud Natural ... analysis to _).

    ```bash
    gsutil cp result.json <UPLOAD_PATH>
    ```

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

---

<!-- Completion Section -->
# Completion

At this point, you should have completed the lab.
