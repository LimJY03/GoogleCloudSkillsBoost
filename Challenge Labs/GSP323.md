# GSP323 (Perform Foundational Data, ML, and AI Tasks in Google Cloud: Challenge Lab)

<!-- Disclaimer Section -->
> **Warning**
> <br>The solutions shown here might not work if there are task changes in the future.

![last_updated_16062022](https://img.shields.io/badge/last%20updated-16%20June%202022-red)

<!-- Lab Description With Link -->
This challenge lab tests your skills and knowledge from the labs in the Baseline Data, ML and AI, quest. You should be familiar with the content of the labs before attempting this lab.

Link to the challenge lab: [click here](https://www.cloudskillsboost.google/focuses/11044?parent=catalog)

## Challenge Scenario 

As a junior data engineer in Jooli Inc. and recently trained with Google Cloud and a number of data services you have been asked to demonstrate your newly learned skills. The team has asked you to complete the following tasks.

---

<!-- Task and Solution -->
# Task 1: Run A Simple Dataflow Job

1. Ensure that the necessary Dataflow API is enabled, restart the connection to the Dataflow API by following the steps below:
    * In the Cloud Console, enter "Dataflow API" in the top search bar. Click on the result for **Dataflow API**.
    * Click **Manage** > **Disable API** > **Disable**.
    * After the API is disabled, click **Enable**.
2. Open Cloud Shell and execute to create a BigQuery data set, replacing `<BIG_QUERY_DATASET_NAME>` with the name given in the task ("Create a BigQuery dataset called _").

    ```bash
    bq mk <BIG_QUERY_DATASET_NAME>
    ```

3. Then, execute the following code to obtain the schema of the data set.

    ```bash
    gsutil cp gs://cloud-training/gsp323/lab.schema .
    ```

    ```bash
    cat lab.schema
    ```

    This will open up the schema for the data set. We will need to copy it later on.
4. To create BigQuery data set, type "BigQuery" in the search bar to enter BigQuery. Then, click on the project ID at the left and select the created data set.
5. Click **CREATE TABLE** and enter each field with the following:
    * Under **Source**, set each of the field to:
        * **Create table from**: `Google Cloud Storage`
        * **Select file from GCS bucket**: `gs://cloud-training/gsp323/lab.csv`

            > **Note**
            > <br>If error occurs, then paste the path after '`gs://`' only.

        * **File format**: `CSV`
    * Under **Destination**, set each of the field to:
        * Set the **Table Name** to the one behind the final `.` given in the task (BigQuery Output Table). For example: `customer_164`
    * Under **Schema**, toggle on **Edit as text**, and paste the schema from Cloud Shell to the field below. It should look like this:

        ```json
        [
            {"type":"STRING","name":"guid"},
            {"type":"BOOLEAN","name":"isActive"},
            {"type":"STRING","name":"firstname"},
            {"type":"STRING","name":"surname"},
            {"type":"STRING","name":"company"},
            {"type":"STRING","name":"email"},
            {"type":"STRING","name":"phone"},
            {"type":"STRING","name":"address"},
            {"type":"STRING","name":"about"},
            {"type":"TIMESTAMP","name":"registered"},
            {"type":"FLOAT","name":"latitude"},
            {"type":"FLOAT","name":"longitude"}
        ]
        ```

    Then, click **Create Table**.
6. To create a bucket, type "Storage" to enter Cloud Storage. Then, click **Create a Storage Bucket** and enter the **Name** as provided in the task ("Create a Cloud Storage Bucket called _").
7. Next, type "Dataflow Jobs" at the search bar to enter the Job tab of Dataflow. Then, click **Create Job from Template**.
8. Then for **Dataflow Template**, choose **Text Files on Cloud Storage to BigQuery** under "Process Data in Bulk (batch)", leave **Job Name** and **Regional Endpoint** as default. Under **Required Parameters**, follow the steps below.

    > **Note**
    > <br>Make sure that the chosen "Text Files on Cloud Storage to BigQuery" is under "Process Data in Bulk (batch)", not "Process Data Continuously (Stream)".

    > **Note**
    > <br>For the following URLs, ignore '`gs://`' if it is provided in the field.already.

    * At "Job name", type any name as you want.
    * At "JavaScript UDF ... Storage", type `gs://cloud-training/gsp323/lab.js`.
    * At "JSON path", type `gs://cloud-training/gsp323/lab.schema`.
    * At "JavaScript UDF name", type `transform`.
    * At "BigQuery output table", type the name given in the task (BigQuery Output Table).
    * At "Cloud storage input path", type `gs://cloud-training/gsp323/lab.csv`
    * At "Temporary directory ... process", paste the directory given in the task (Temporary BigQuery directory).
    * At "Temporary location", paste the directory given in the task (Temporary location).

    Then, click **Run Job**. After the job has completed running, proceed to the next step.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

---

# Task 2: Run A Simple Dataproc Job

1. Type "Dataproc" in the search bar to enter Dataproc. In **Clusters** tab, click **Create Cluster** at the middle of the page. 
    * For "Region", enter the region given in the task (Region). 
    
    Leave the others at default and click **Create**.
2. Select the created cluster, then click **VM Instances** tab. Then, click the **SSH** button under the "master" instance.
3. On the pop-up SSH window, enter the following code.

    ```bash
    hdfs dfs -cp gs://cloud-training/gsp323/data.txt /data.txt
    ```

    > **Note**
    > <br>This must be done in the SSH pop-up window, not in Cloud Shell.

4. Exit the SSH and click **Submit Job** near the top of the page. Enter each field with the following.
    * At "Job ID", leave it.
    * At "Job Type", choose **Spark**.
    * At "Main class or jar", type `org.apache.spark.examples.SparkPageRank`.
    * At "Jar files", type `file:///usr/lib/spark/examples/jars/spark-examples.jar`.
    * At "Archive files", leave it.
    * At "Arguments", type `/data.txt`.
    * At "Max restarts per hour", type `1`.

    Then, click **Submit**. After the job has completed running, proceed to the next step.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 3: Run A Simple Dataprep Job

1. Type "Dataprep" at the search bar to enter Dataprep.
2. Accept everything from the Terms of Services at the pop-up menu until a alert prompt asked you to leave site, then click **Leave Site**.
3. Sign in to Dataprep using your Qwiklab student account (not your own account). Then continue to accept everything.
4. Click **Import Data** near the top right of the screen, then select **Cloud Storage**.
5. Click on the pencil icon beside Cloud Storage to edit the path, then paste `gs://cloud-training/gsp323/runs.csv` into the path. Then, click **Go**.

    > **Note**
    > <br>For the following URLs, ignore '`gs://`' if it is provided in the field.already.

6. Click **Run Flow** and you will enter the "spreadsheet" view of the data. Click **Add New Step** at the recipe at the right of the screen.
7. Click the bar that shows "FAILURE" in column 10, then select "Delete rows where ..." at the recipe at the right, then click **Add**.
8. Click **New Step** and **Add** after each section completes. Perform the following sections repeatedly:
    * Search "Contains" and click **Filter Contains**. Then, choose **Column 9**. For "Pattern", type `/(^0$|^0\.0$)/`. Then click **Delete matching rows**.
    * Search "Rename" and click **Rename Columns**. Then, do the following:
        * Select **Column 2** and type `'runid'` at "New name". Then, click the `+` icon below the "New name" field to add new rename function.
        * Select **Column 3** and type `'userid'` at "New name". Then, click the `+` icon below the "New name" field to add new rename function.
        * Select **Column 4** and type `'labid'` at "New name". Then, click the `+` icon below the "New name" field to add new rename function.
        * Select **Column 5** and type `'lab_title'` at "New name". Then, click the `+` icon below the "New name" field to add new rename function.
        * Select **Column 6** and type `'start'` at "New name". Then, click the `+` icon below the "New name" field to add new rename function.
        * Select **Column 7** and type `'end'` at "New name". Then, click the `+` icon below the "New name" field to add new rename function.
        * Select **Column 8** and type `'time'` at "New name". Then, click the `+` icon below the "New name" field to add new rename function.
        * Select **Column 9** and type `'score'` at "New name". Then, click the `+` icon below the "New name" field to add new rename function.
        * Select **Column 3** and type `'state'` at "New name". Then, click **Add** at the bottom right to add all functions to the recipe.
9. Click **Run** at the top right of the screen. Then click **Run** at the bottom right of the screen. 
10. After the job has completed running, proceed to the next step.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Task 4: AI

For this section, choose one of the AI tasks given in the challenge lab task. The code below is for the question on **Cloud Natural Language API**.

1. In Cloud Shell, run the following code one-by-one.

    ```bash
    gcloud iam service-accounts create my-natlang-sa \
      --display-name "my natural language service account"

    gcloud iam service-accounts keys create ~/key.json \
      --iam-account my-natlang-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com

    export GOOGLE_APPLICATION_CREDENTIALS="/home/$USER/key.json"

    gcloud auth activate-service-account my-natlang-sa@${GOOGLE_CLOUD_PROJECT}.iam.gserviceaccount.com --key-file=$GOOGLE_APPLICATION_CREDENTIALS

    gcloud ml language analyze-entities --content="Old Norse texts portray Odin as one-eyed and long-bearded, frequently wielding a spear named Gungnir and wearing a cloak and a broad hat." > result.json
    ```

3. Execute the following part, type 'y' and a link will be generated.

    ```bash
    gcloud auth login
    ```

    Click the link and authorize everything, copy a code generated, then return to the Cloud Shell and paste the code at the prompt.

4. Replace the `<UPLOAD_PATH>` with the url given in the task (Use the Cloud Natural ... analysis to _).

    ```bash
    gsutil cp result.json <UPLOAD_PATH>
    ```

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

---

<!-- Completion Section -->
# Completion

At this point, you should have completed the lab.