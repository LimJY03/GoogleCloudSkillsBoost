# GSP 324 (Explore Machine Learning Models with Explainable AI: Challenge Lab)

<!-- Disclaimer Section -->
> **Warning**
> <br>The solutions shown here might not work if there are task changes in the future.

![last_updated_15062022](https://img.shields.io/badge/last%20updated-15%20June%202022-red)

<!-- Lab Description With Link -->
In this challenge lab you'll apply knowledge of AI Platform and the What-If Tool to identify and address bias in datasets.

Link to the challenge lab: [click here](https://www.cloudskillsboost.google/focuses/12011?parent=catalog)

## Challenge Scenario 

You are a curious coder who wants to explore biases in public datasets using the What-If Tool. You decide to pull some mortgage data to train a couple of machine learning models to predict whether an applicant will be granted a loan. 

You specifically want to investigate how the two models perform when they are trained on different proportions of males and females in the datasets, and visualize their differences in the What-If Tool.

---

<!-- Task and Solution -->
# Start A JupyterLab Notebook Instance

> **Note**
> <br>For this task, follow all steps in the challenge lab.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Download The Challenge Notebook

> **Note**
> <br>For this task, follow all steps in the challenge lab.
>
> The download and import of `hmda_2017_ny_all-records_labels` data sets are in the code blocks under "**Download and import the data**". Execute the blocks before you proceed to <ins>**Check Your Progress**</ins>.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Build And Train Your Models

1. Read all columns starting from "**Process the Data**" and execute the code blocks accordingly.
2. Upon reaching "**Create and train your TensorFlow models**", you will need to start solving the challenge. 
3. Execute the code block containing `# import TF modules`.
4. For the next code block containing `# Train the first model ...`, add the following codes to the below of all commented-lines.

    ```py
    model = Sequential()

    # the size of array from the 2nd line in this code block is our input dimension
    model.add(Dense(8, input_shape=(input_size,)))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='sgd', loss='mse')

    # train_data and train_labels are x and y from the splitted data from above cells.
    model.fit(train_data, train_labels, batch_size=32, epochs=10)
    ```

5. Execute this code block and the two code blocks below to save and get predictions with this model #1.
6. For the next code block containing `# Train the second model ...`, add the following codes to the below of all commented-lines.

    ```py
    limited_model = Sequential()

    # the input dimension is still the same 
    limited_model.add(Dense(8, input_shape=(input_size,)))
    limited_model.add(Dense(1, activation='sigmoid'))
    limited_model.compile(optimizer='sgd', loss='mse')

    # The first 2 arguments are x and y from the splitted bad data from above cells.
    limited_model.fit(limited_train_data, limited_train_labels, batch_size=32, epochs=10)
    ```

7. Execute this code block and the two code blocks below to save and get predictions with this model #2.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Deploy The Models To AI Platform

1. Back to Cloud Console, search "storage" in the search bar and click **Cloud Storage**. Then, click **CREATE BUCKET** and leave everything as it is, then click **Create**.
2. Back to JupyterLab, do the following value replacement.

    * Replace the value `'#TODO'` for `GCP_PROJECT` with your Qwiklabs Project ID.
    * Replace `#TODO` in the URL for `MODEL_BUCKET` with your Qwiklabs Project ID. 
        > **Note**
        > <br>Make sure that there is **NO whitespace** in the url.
    * Replace `'saved_complete_model'` for `MODEL_NAME` with the "Model Name" under "... first AI Platform model:" given in the task (ignore the `#do not modify` in Jupyter notebook).
    * Replace `'saved_limited_model'` for `LIM_MODEL_NAME` with the "Model Name" under "... second AI Platform model:" given in the task (ignore the `#do not modify` in Jupyter notebook).

3. Run the three code blocks in this section.
4. At the line "Navigate back to the ..." under "**Create your first AI Platform model: saved_complete_model**", click "+" at the menu bar two times to add 2 empty code blocks, and add the following code into the first created code block.

    ```bash
    !gcloud ai-platform models create $MODEL_NAME --regions $REGION
    ```

5. Add the following code into the second created code block.

    ```bash
    !gcloud ai-platform versions create $VERSION_NAME\
    --model=$MODEL_NAME \
    --framework='TENSORFLOW' \
    --runtime-version=2.3 \
    --origin='./saved_complete_model' \
    --staging-bucket=$MODEL_BUCKET \
    --python-version=3.7 \
    --project=$GCP_PROJECT
    ```

    > **Note**
    > <br>1. The `runtime-version` gets value from "ML Runtime version" provided in the task.
    > <br>2. The `python-version` gets value from "Python version" provided in the task.
    > 
    > Check if both values used in the code block are same with the ones provided.

6. At the line "Navigate back to the ..." under "**Create your second AI Platform model: saved_limited_model**", click "+" at the menu bar two times to add 2 empty code blocks, and add the following code into the first created code block.

    ```bash
    !gcloud ai-platform models create $LIM_MODEL_NAME --regions $REGION
    ```

7. Add the following code into the second created code block.

    ```bash
    !gcloud ai-platform versions create $VERSION_NAME\
    --model=$LIM_MODEL_NAME \
    --framework='TENSORFLOW' \
    --runtime-version=2.3 \
    --origin='./saved_limited_model' \
    --staging-bucket=$MODEL_BUCKET \
    --python-version=3.7 \
    --project=$GCP_PROJECT
    ```

8. Execute all of the 4 created code blocks orderly.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Use The What-If Tool To Explore Biases

For this section, just copy the code shown below (or copy from the one given in the task) and paste to the below of `# Add WitConfig ...` in the `ipynb`.

```py
config_builder = (WitConfigBuilder(
    examples_for_wit[:num_datapoints],feature_names=column_names)
    .set_custom_predict_fn(bad_custom_predict)
    .set_target_feature('loan_granted')
    .set_label_vocab(['denied', 'accepted'])
    .set_compare_custom_predict_fn(custom_predict)
    .set_model_name('limited')
    .set_compare_model_name('complete'))

WitWidget(config_builder, height=800)
```

Then, execute this code block.

```diff
@@ REMEMBER TO CHECK YOUR PROGRESS @@
```

# Review Your Results

Back to the tasks, answer the questions below using the results from your model. Below are my answers:

1. In the Performance and Fairness tab, slice by sex (applicant_sex_name_Female). How does the complete model compare to the limited model for females?

    * [X] **The complete model has equal performance across sexes, whereas the limited model is much worse on females**
    * [ ] The limited model is the same as the complete model, and there is roughly no difference between the two
    * [ ] The complete model has better performance on males over the limited model
    * [ ] The complete model has unequal performance across sexes, whereas the limited model is better on females

2. Click on one of the datapoints in the middle of the arc. In the datapoint editor, change (applicant_sex_name_Female) to 0, and (applicant_sex_name_Male) to 1. Now run the inference again. How does the model change?


    * [ ] The limited model and the complete model both have large and noticeable deltas
    * [X] **The limited model has a significantly larger delta than the complete model, whereas the complete model has almost no change**
    * [ ] Changing the sex of the data point has no discernible effect on either of the models
    * [ ] The limited model has around the same delta as the complete model

3. In the Performance and Fairness tab, use the fairness buttons to see the thresholds for the sexes for demographic parity between males and females. How does this change the thresholds for the limited model?


    * [X] **The thresholds have to be wildly different for the limited model**
    * [ ] The effect of changing the optimization strategy does not have a noticeable effect on the thresholds
    * [ ] The thresholds stay mostly consistent between the models
    * [ ] The thresholds have to be wildly different for the complete model

---

<!-- Completion Section -->
# Completion

At this point, you should have completed the lab.