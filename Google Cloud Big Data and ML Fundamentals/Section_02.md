# Data Engineering for Streaming Data

Batch processing is when the processing and analysis happens on a set of stored data, like payroll and billing systems that have to be processed on either a weekly or monthly basis.

Data streaming is a flow of data records generated by various data sources. The processing of streaming data happens as the data flows through a system. This results in the analysis and reporting of events as they happen. An example of the use case is fraud detection or intrusion detection.

Modern data processing has progressed from legacy batch processing of data towards working with real-time data streams. For example, streaming music and movies no longer requires users to download an entire movie or album to a local device.

![batch_vs_stream](https://media.discordapp.net/attachments/984655726406402088/986112419304718396/unknown.png?width=1246&height=701)

In this section, we will explore data engineering for streaming data, with the goal of building a real-time data solution with Google Cloud products and services. This includes

* Ingesting streaming data using Cloud Pub/Sub.
* Process the data using Google Dataflow.
* Visualize the results with Google Data Studio and Looker.

Link to this part can be found at [here](https://youtu.be/Exf9dd1S2AA).

---

# Big Data Challenges

Building scalable and reliable pipelines is a core responsibility of data engineers. However in modern organizations, data engineers and data scientists are facing four major challenges:

![4v](https://media.discordapp.net/attachments/984655726406402088/986112950932758538/unknown.png?width=1246&height=701)

* Data could come in from a variety of different sources and in various formats like number, image, or even audio.
* Data pipeline code and infrastructure should be able to scale with the changes to prevent halt or crashes.
* Data often needs to be processed in near real time as soon as it reaches the system.
    * Data that arrives late, has bad data or needs to be transformed has to be handled as well.
* Veracity refers to data quality. 
    * There's a possibility that gathered data will come with some inconsistencies and uncertainties.

Link to this part can be found at [here](https://youtu.be/yPy7Yb0Jdjc).

# Message-Oriented Architecture

One of the early stages in a data pipeline is data ingestion, which is where large amounts of streaming data are received.

Data may not always come from a single structured database, it might stream from a thousand or even a million different events that are all happening asynchronously.

There are 4 general challenges for data ingestion.

![data_ingestion_challenges](https://media.discordapp.net/attachments/984655726406402088/986115190900813894/unknown.png?width=1248&height=701)

Google Cloud has a tool to handle distributed message-oriented architectures at scale and that's Cloud Pub/Sub (Publisher/Subscriber or Publish messages to Subscribers). 

Cloud Pub/Sub is a distributed messaging service that can receive messages from a variety of device streams such as gaming events, IoT devices and application streams.

![pubsub_features](https://media.discordapp.net/attachments/984655726406402088/986115927533826069/unknown.png?width=1246&height=701)

Upstream source data comes in from devices all over the globe and is ingested into Cloud Pub/Sub, which is the first point of contact within the system. It reads, stores and broadcasts to any subscribers of this data topic, that new messages are available. 

Cloud Pub/Sub can ingest and transform those messages in an elastic streaming pipeline and output the results into an analytics data warehouse like BigQuery. 

Then, users can connect a data visualization tool like Looker or Google Data Studio to visualize and monitor the results of a pipeline, or an AI or ML tool such as Vertex AI to explore the data to uncover business insights or help with predictions.

> *"You can think of a topic like a radio antenna. Whether your radio is playing music or it is turned off, the antenna itself is always there. If music is being broadcast in a frequency that nobody's listening to, the stream of music still exists. Similarly a Publisher can send data to a topic that has no Subscriber to receive it."*

In a fully-operational pipeline, the Publisher is sending data to a topic that an application is Subscribed to. This means that there can be 0, 1 or more Publishers and 0, 1 or more Subscribers related to a topic. 

They are completely decoupled so they are free to break without affecting their counterparts.

![pubsub](https://media.discordapp.net/attachments/984655726406402088/986118308715700294/unknown.png?width=1246&height=701)

Cloud Pub/Sub supports many different inputs and outputs. Users can even publish a Pub/Sub event from one topic to another.

Link to this part can be found at [here](https://youtu.be/76szWPgPf84).

# Designing Streaming Pipelines with Apache Beam

After messages have been captured from the streaming input sources, a way is needed to pipe that data into a data warehouse for analysis. This is where Google Dataflow comes in.

When building a data pipeline, data engineers often encounter challenges related to coding, implementing and serving the pipelines at scale.

![questions_challenges](https://media.discordapp.net/attachments/984655726406402088/986119223283679272/unknown.png?width=1246&height=701)

A popular solution for pipeline design is Apache Beam. It is an open source, unified programming model to define and execute data processing pipelines, including ETL, batch, and stream processing.

Apache Beam is:

* Unified - uses a single programming model for both batch and streaming data.
* Portable - can work on multiple execution environments.
* Extensible - allows users to write and share their own connectors and transformation libraries.

Apache Beam has:

* Pipeline Templates - users don't need to build a pipeline from scratch.
* SDK - provides a variety of libraries for transformations and data connectors to sources and sinks.
* Model Representation - Apache Beam creates a model representation from user's code that's portable across many runners.
    * Runners pass off user's model for execution on a variety of different engines like Google Dataflow.

Link to this part can be found at [here](https://youtu.be/AiqeMxpq1Ik).

# Implementing Streaming Pipelines on Google Dataflow

When choosing an execution engine for pipeline code, it may be helpful to consider the following questions.

![pipeline_questions](https://media.discordapp.net/attachments/984655726406402088/986120808571236382/unknown.png?width=1246&height=701)

Google Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud ecosystem.

* Google Dataflow handles much of the complexity relating to infrastructure setup and maintenance.
* Google Dataflow is built on Google's infrastructure.
    * This allows for reliable auto-scaling to meet data pipeline demands.
* Google Dataflow is serverless and NoOps (No Operations).

![noops_serverless](https://media.discordapp.net/attachments/984655726406402088/986121845554163744/unknown.png?width=1246&height=701)

Using serverless NoOps solutions like Google Dataflow means that users can spend more time analyzing the insights from their data sets and less time provisioning resources to ensure that the pipeline will successfully complete its next cycles.

Below are the tasks Google Dataflow performs when a job is received.

![dataflow_task](https://media.discordapp.net/attachments/984655726406402088/986122534170791946/unknown.png?width=1246&height=701)

Google Dataflow templates covers common use cases across Google Cloud products. The list of templates is continuousl growing, and they can be broken down into three categories:

* Streaming Templates - For processing continuous or real-time data.
* Batch Templates - For processing bulk data or batch load data.
* Utility Templates - Address activities related to bulk compression deletion and conversion.

Link to this part can be found at [here](https://youtu.be/fzP6mUK_Tfw).

# Visualization with Looker

Telling a good story with data through a dashboard can be critical to the success of a data pipeline. Data that's difficult to interpret or draw insights from might be useless.

To help create an environment where stakeholders can easily interact with and visualize data, Google Cloud offers two solutions:

* Looker
* Google Data Studio

![looker_supports](https://media.discordapp.net/attachments/984655726406402088/986124276652462080/unknown.png?width=1246&height=701)

Looker supports BigQuery as well as more than 60 different types of SQL database products commonly referred to as dialects. It allows developers to define a semantic modeling layer on top of databases using Looker Modeling Language or LookML.

![lookml](https://media.discordapp.net/attachments/984655726406402088/986124477437997107/unknown.png?width=1252&height=701)

The Looker platform is 100 web-based, which makes it easy to integrate into existing workflows, and share with multiple teams at an organization. There's also a Looker API which can be used to embed Looker reports in other applications.

## Dashboard

Dashboards like the [Business Pulse Dashboard](https://marketplace.looker.com/marketplace/detail/business-pulse-by-quantum-metric) can visualize data in a way that makes insights easy to understand.

Based on the metrics that are important to their business, users can create Looker dashboards that provide straight-forward presentations to help them and their colleagues to quickly see a high level business status. 

Looker has multiple data visualization options. It also allow users to plot data on a map.

![looker_graphs](https://media.discordapp.net/attachments/984655726406402088/986125747573915738/unknown.png?width=1246&height=701)

To share a dashboard with the team, users just need to schedule delivery through storage services like Google Drive, Slack and Dropbox.

Link to this part can be found at [here](https://youtu.be/duQzCa51AlY).

# Visualization with Google Data Studio

Google Data Studio is integrated into BigQuery, which makes data visualization possible with just a few clicks. This means that leveraging Data Studio doesn't require support from an administrator to establish a data connection, which is a requirement with Looker.

Data Studio dashboards are widely used across many Google products and applications .

![ganalytics_dashboard](https://media.discordapp.net/attachments/984655726406402088/986126743607853097/unknown.png?width=1246&height=701)

Another Data Studio integration is the Google Cloud billing dashboard. It is commonly used to monitor spending.

## Steps to Create Data Studio Dashboards

1. Choose a Template
    * Users can start either with a pre-built template or from a blank report.
2. Link Dashboard to Data Sources
    * The data might come from BigQuery, a local file or a Google application like Google Sheets or Google Analytics.
3. Explore The Dashboard

Link to this part can be found at [here](https://youtu.be/8BJy0IttYSo).

# Lab: Creating Streaming Data Pipeline for Real-Time Dashboard with Dataflow

In this lab, we will:

* Create Cloud Dataflow job from a template.
* Subscribe to a Pub/Sub topic.
* Streaming a Cloud Dataflow pipeline into BigQuery and monitor it.
* Analyze results with SQL.
* Visualize key metrics in Data Studio

Link to this part can be found at [here](https://youtu.be/P1qoNccjCXU).
<br>Link to the lab at [here](https://www.cloudskillsboost.google/course_sessions/1170736/labs/200019).
<br>Link to the data set used at [here](https://github.com/LimJY03/GoogleCloudSkillsBoost/blob/main/Google%20Cloud%20Big%20Data%20and%20ML%20Fundamentals/Datasets/taxirides-realtime/nyc_taxirides.csv).

---

# Section Quiz

1. Due to several data types and sources, big data often has many data dimensions. This can introduce data inconsistencies and uncertainties. Which type of challenge might this present to data engineers?

* [X] **Veracity**
* [ ] Variety
* [ ] Volume
* [ ] Velocity

2. Select the correct streaming data workflow.

* [ ] Process the data, visualize the data, and ingest the data.
* [X] **Ingest the streaming data, process the data, and visualize the results.**
* [ ] Visualize the data, process the data, and ingest the streaming data.
* [ ] Ingest the streaming data, visualize the data, and process the data.

3. When you build scalable and reliable pipelines, data often needs to be processed in near-real time, as soon as it reaches the system. Which type of challenge might this present to data engineers?

* [ ] Veracity
* [ ] Variety
* [ ] Volume
* [X] **Velocity**

4. Which Google Cloud product acts as an execution engine to process and implement data processing pipelines?

* [X] **Dataflow**
* [ ] Apache Beam
* [ ] Looker
* [ ] Data Studio

5. Which Google Cloud product is a distributed messaging service that is designed to ingest messages from multiple device streams such as gaming events, IoT devices, and application streams?

* [ ] Apache Beam
* [ ] Looker
* [X] **Pub/Sub**
* [ ] Data Studio

---

# Section Summary

Link to this part can be found at [here](https://youtu.be/0Ax0_JZ6D1c).

## Recommended Reading List

Below are some reading list on this section suggested by this course on Google Cloud.

* [Pub/Sub documentation](https://cloud.google.com/pubsub/docs/)
* [Dataflow documentation](https://cloud.google.com/dataflow/docs/) and [Dataflow templates](https://cloud.google.com/dataflow/docs/concepts/dataflow-templates)
* [Data Studio documentation](https://developers.google.com/datastudio/) and [Data Studio templates](https://datastudiogallery.appspot.com/gallery)
* [Looker user guide](https://connect.looker.com/) and [Looker documentation](https://docs.looker.com/)