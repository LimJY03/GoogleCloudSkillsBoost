{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86d526a-5fbc-42d1-934c-ef8c0a2acaee",
   "metadata": {},
   "source": [
    "# Vertex Pipelines: Qwik Start\n",
    "\n",
    "Pipelines help you automate and reproduce your ML workflow. Vertex AI integrates the ML offerings across Google Cloud into a seamless development experience. \n",
    "\n",
    "Previously, models trained with AutoML and custom models were accessible via separate services. Vertex AI combines both into a single API, along with other new products. Vertex AI also includes a variety of MLOps products, like Vertex Pipelines. \n",
    "\n",
    "In this lab, you will learn how to create and run ML pipelines with Vertex Pipelines.\n",
    "\n",
    "## Why Are ML Pipelines Useful?\n",
    "\n",
    "Before diving in, first understand why you would want to use a pipeline. \n",
    "\n",
    "Imagine you're building out a ML workflow that includes processing data, training a model, hyperparameter tuning, evaluation, and model deployment. Each of these steps may have different dependencies, which may become unwieldy if you treat the entire workflow as a monolith. \n",
    "\n",
    "As you begin to scale your ML process, you might want to share your ML workflow with others on your team so they can run it and contribute code. Without a reliable, reproducible process, this can become difficult. \n",
    "\n",
    "With pipelines, each step in your ML process is its own container. This lets you develop steps independently and track the input and output from each step in a reproducible way. You can also schedule or trigger runs of your pipeline based on other events in your Cloud environment, like when new training data is available.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Use the Kubeflow Pipelines SDK to build scalable ML pipelines.\n",
    "* Create and run a 3-step intro pipeline that takes text input.\n",
    "* Create and run a pipeline that trains, evaluates, and deploys an AutoML classification model.\n",
    "* Use pre-built components for interacting with Vertex AI services, provided through the google_cloud_pipeline_components library.\n",
    "* Schedule a pipeline job with Cloud Scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8dad74-a9ab-4fae-a5bc-1f6f1075986c",
   "metadata": {},
   "source": [
    "# Vertex Pipelines Setup\n",
    "\n",
    "There are a few additional libraries you'll need to install in order to use Vertex Pipelines:\n",
    "\n",
    "* **Kubeflow Pipelines**: This is the SDK used to build the pipeline. Vertex Pipelines supports running pipelines built with both Kubeflow Pipelines or TFX.\n",
    "* **Google Cloud Pipeline Components**: This library provides pre-built components that make it easier to interact with Vertex AI services from your pipeline steps.\n",
    "\n",
    "## Install Libraries\n",
    "\n",
    "To install both services needed for this lab, first set the user flag in a notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec35a14-57e1-4501-8bbc-eed822a2aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4aa5c75-78af-4e3a-bac5-dc630f7a6df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform==1.0.0\n",
      "  Downloading google_cloud_aiplatform-1.0.0-py2.py3-none-any.whl (1.8 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.8 MB 5.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.0.0) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.0.0) (2.30.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.0.0) (1.19.8)\n",
      "Collecting google-api-core[grpc]<2.0.0dev,>=1.22.2\n",
      "  Downloading google_api_core-1.31.6-py2.py3-none-any.whl (93 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93 kB 2.3 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform==1.0.0) (1.43.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2021.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.53.0)\n",
      "Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (3.19.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2.26.0)\n",
      "Collecting google-auth<2.0dev,>=1.25.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152 kB 55.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (59.4.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.42.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.1.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform==1.0.0) (3.0.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (4.2.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (1.1.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (1.26.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform==1.0.0) (0.4.8)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.0.0) (2.21)\n",
      "Installing collected packages: google-auth, google-api-core, google-cloud-aiplatform\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.31.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-api-core-1.31.6 google-auth-1.35.0 google-cloud-aiplatform-1.0.0\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.12.tar.gz (301 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 301 kB 4.7 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-cloud-pipeline-components==0.1.1\n",
      "  Downloading google_cloud_pipeline_components-0.1.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform>=1.0.0 in ./.local/lib/python3.7/site-packages (from google-cloud-pipeline-components==0.1.1) (1.0.0)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.7/site-packages (from kfp) (0.15.0)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 636 kB 76.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in ./.local/lib/python3.7/site-packages (from kfp) (1.31.6)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.43.0)\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 61.8 MB/s            \n",
      "\u001b[?25hCollecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62 kB 929 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.1 in ./.local/lib/python3.7/site-packages (from kfp) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54 kB 3.8 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (2.0.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.2.tar.gz (57 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 57 kB 8.4 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56 kB 6.5 MB/s             \n",
      "\u001b[?25hCollecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (8.0.3)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.14.1-py3-none-any.whl (33 kB)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.14\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87 kB 8.5 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.19.1)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.0.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.8.2)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.5.0-py3-none-any.whl (28 kB)\n",
      "Collecting typing-extensions<4,>=3.7.4\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp) (4.8.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (59.4.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2021.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.26.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (21.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.20.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.8)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.0.0->google-cloud-pipeline-components==0.1.1) (1.19.8)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform>=1.0.0->google-cloud-pipeline-components==0.1.1) (2.30.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.2.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage<2,>=1.20.0->kfp) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2.8.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.2.1)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (1.42.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp) (3.0.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp) (3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp) (2.21)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.8.12-py3-none-any.whl size=419048 sha256=c97623cd685b2255664c5972b14ce8887ec3d4edd91577c62b32012905d25125\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/54/0c/4a/3fc55077bc88cc17eacaae34c5fd3f6178c1d16d2ee3b0afdf\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=85dc524ee25ad9166b73266e91279670f4e7df7de8d6713d1ef7cf2aadf427eb\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.2-py3-none-any.whl size=99719 sha256=32bdef0f50e29430db5df283600431044ae48381145de04de64bd6639e9580f1\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/77/36/d3/60e33cc9e15f269fe0e0f71cae6d077a5e43973d514b60b4ad\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=51b4e66420565a3888a7af4a713880c3fabee49a6495e9dbe1d71f27cda783a8\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: typing-extensions, PyYAML, typer, tabulate, strip-hints, requests-toolbelt, kubernetes, kfp-server-api, kfp-pipeline-spec, jsonschema, google-api-python-client, fire, docstring-parser, Deprecated, kfp, google-cloud-pipeline-components\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script jsonschema is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "tfx-bsl 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\n",
      "tfx-bsl 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "tfx-bsl 1.4.0 requires tensorflow-metadata<1.5,>=1.4, but you have tensorflow-metadata 1.5.0 which is incompatible.\n",
      "tensorflow 2.6.2 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow 2.6.2 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
      "tensorflow 2.6.2 requires wrapt~=1.12.1, but you have wrapt 1.13.3 which is incompatible.\n",
      "tensorflow-transform 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\n",
      "tensorflow-transform 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
      "tensorflow-transform 1.4.0 requires tensorflow-metadata<1.5.0,>=1.4.0, but you have tensorflow-metadata 1.5.0 which is incompatible.\n",
      "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.11 which is incompatible.\n",
      "apache-beam 2.34.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\n",
      "apache-beam 2.34.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.2 which is incompatible.\n",
      "apache-beam 2.34.0 requires pyarrow<6.0.0,>=0.15.1, but you have pyarrow 6.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 docstring-parser-0.14.1 fire-0.4.0 google-api-python-client-1.12.11 google-cloud-pipeline-components-0.1.1 jsonschema-3.2.0 kfp-1.8.12 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.2 kubernetes-18.20.0 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.10 typer-0.5.0 typing-extensions-3.10.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform==1.0.0 --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components==0.1.1 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcdda7-6666-472e-a028-d582bb09cfc6",
   "metadata": {},
   "source": [
    "After installing these packages you'll need to restart the kernel before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfba8b1f-b54a-4292-a4fe-c9de1b43794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Automatically restart kernel after installs\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9517cd9-033e-41a5-a0c1-26da15d38c7e",
   "metadata": {},
   "source": [
    "Finally, check that you have correctly installed the packages. The KFP SDK version should be >=1.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da501955-4412-4940-bf4b-7a7b30842003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.12\n",
      "google_cloud_pipeline_components version: 0.1.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "!python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f919c-fa7c-4b55-8912-874037768f9c",
   "metadata": {},
   "source": [
    "## Step 2: Set Project ID and Bucket\n",
    "\n",
    "Now, we'll create variables for each Project ID and Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d567f6-20f8-4aa1-b06b-b0767798bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"qwiklabs-gcp-01-33949882a483\"\n",
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba1dfb-75ac-43a0-9885-ed13db964b1e",
   "metadata": {},
   "source": [
    "## Step 3: Import libraries\n",
    "\n",
    "Import the libraries will be used throughout this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0d63db-df47-49f6-b41b-973b7d4e3e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba530a2-13e3-4de0-8c0f-6aa1b2a0b8be",
   "metadata": {},
   "source": [
    "## Step 4: Define Constants\n",
    "\n",
    "The last thing we need to do before building the pipeline is define some constant variables. `PIPELINE_ROOT` is the Cloud Storage path where the artifacts created by our pipeline will be written. We're using `us-central1` as the region here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa8ec339-018d-43f5-80fa-2a956ae14173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://qwiklabs-gcp-01-33949882a483-bucket/pipeline_root/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fde862-0641-42b2-b801-817b8c61053c",
   "metadata": {},
   "source": [
    "After running the code above, you should see the root directory for your pipeline printed. This is the Cloud Storage location where the artifacts from your pipeline will be written. It will be in the format of `gs://<bucket_name>/pipeline_root/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b17880-7048-47f4-8c72-9f3bb49c1be7",
   "metadata": {},
   "source": [
    "# Creating Our First Pipeline\n",
    "\n",
    "Create a short pipeline using the KFP SDK. This pipeline doesn't do anything ML related, this exercise is to teach us:\n",
    "\n",
    "* How to create custom components in the KFP SDK.\n",
    "* How to run and monitor a pipeline in Vertex Pipelines.\n",
    "\n",
    "We'll create a pipeline that prints out a sentence using two outputs: a product name and an emoji description. This pipeline will consist of three components:\n",
    "\n",
    "1. `product_name`: This component will take a product name as input, and return that string as output.\n",
    "2. `emoji`: This component will take the text description of an emoji and convert it to an emoji. \n",
    "    * For example, the text code for âœ¨ is \"sparkles\". This component uses an emoji library to show you how to manage external dependencies in your pipeline.\n",
    "3. `build_sentence`: This final component will consume the output of the previous two to build a sentence that uses the emoji. \n",
    "    * For example, the resulting output might be \"Vertex Pipelines is âœ¨\".\n",
    "\n",
    "## Step 1: Create A `Python` Function-Based Component\n",
    "\n",
    "Using the KFP SDK, we can create components based on `Python` functions. First build the `product_name` component, which simply takes a string as input and returns that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c033ef-ba10-4a25-8522-24bc2fa0af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=\"python:3.9\", output_component_file=\"first-component.yaml\")\n",
    "def product_name(text: str) -> str:\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f3a36-0968-4ead-9bd3-e1a90cdef676",
   "metadata": {},
   "source": [
    "Take a closer look at the syntax here:\n",
    "\n",
    "* The `@component` decorator compiles this function to a component when the pipeline is run. We'll use this anytime we write a custom component.\n",
    "* The `base_image` parameter specifies the container image this component will use.\n",
    "* The `output_component_file` parameter is optional, and specifies the `yaml` file to write the compiled component to. \n",
    "\n",
    "After running the cell above, we should see that file written to our notebook instance. If we wanted to share this component with someone, we could send them the generated `yaml` file and have them load it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7fee02b-4ebf-493b-aa55-c7b1c76ddc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_name_component = kfp.components.load_component_from_file('./first-component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946375a6-9806-4db9-9208-25cd90a907e5",
   "metadata": {},
   "source": [
    "The `-> str` after the function definition specifies the output type for this component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862eae2c-8f46-4a0c-8a9f-1618ab6518ec",
   "metadata": {},
   "source": [
    "## Step 2: Create 2 Additional Components\n",
    "\n",
    "To complete the pipeline, create two more components. The first one takes a string as input, and converts this string to its corresponding emoji if there is one. It returns a tuple with the input text passed, and the resulting emoji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feeb6dc9-2e27-410a-bf78-e44a3e5d01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"emoji\"])\n",
    "def emoji(text: str) -> NamedTuple(\"Outputs\", [(\"emoji_text\", str), (\"emoji\", str)]):\n",
    "    \n",
    "    import emoji\n",
    "    \n",
    "    emoji_text = text\n",
    "    emoji_str = emoji.emojize(':' + emoji_text + ':', use_aliases=True)\n",
    "    \n",
    "    print(\"output one: {}; output_two: {}\".format(emoji_text, emoji_str))\n",
    "    \n",
    "    return (emoji_text, emoji_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81ac28-eca4-4a66-bdb5-ce91dbb2b879",
   "metadata": {},
   "source": [
    "This component is a bit more complex than the previous one. Here's what's new:\n",
    "\n",
    "* The `packages_to_install` parameter tells the component any external library dependencies for this container. In this case, we're using a library called emoji.\n",
    "* This component returns a `NamedTuple` called `Outputs`. Notice that each of the strings in this tuple have keys: `emoji_text` and `emoji`. We'll use these in our next component to access the output.\n",
    "\n",
    "The final component in this pipeline will consume the output of the first two and combine them to return a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f28faba-4da1-4d9e-afa9-f68797d8deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "def build_sentence(product: str, emoji: str, emojitext: str) -> str:\n",
    "    \n",
    "    print(\"We completed the pipeline, hooray!\")\n",
    "    \n",
    "    end_str = product + \" is \"\n",
    "    end_str += emoji if (len(emoji) > 0) else emojitext\n",
    "    \n",
    "    return(end_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c2308-1632-4e34-870a-0d0e49851ca2",
   "metadata": {},
   "source": [
    "## Step 3: Putting The Components Together Into A Pipeline\n",
    "\n",
    "The component definitions defined above created factory functions that can be used in a pipeline definition to create steps. To set up a pipeline, use the `@dsl.pipeline` decorator, give the pipeline a name and description, and provide the root path where our pipeline's artifacts should be written. By artifacts, it means any output files generated by our pipeline. This intro pipeline doesn't generate any, but our next pipeline will.\n",
    "\n",
    "In the next block of code we will define an `intro_pipeline` function. This is where we specify the inputs to your initial pipeline steps, and how steps connect to each other:\n",
    "\n",
    "* `product_task` takes a product name as input. Here we're passing \"Vertex Pipelines\" but we can change this to whatever we'd like.\n",
    "* `emoji_task` takes the text code for an emoji as input. We can also change this to whatever we'd like. \n",
    "    * For example, \"party_face\" refers to the ðŸ¥³ emoji. \n",
    "    * Note that since both this and the `product_task` component don't have any steps that feed input into them, we manually specify the input for these when we define our pipeline.\n",
    "* The last step in the pipeline - `consumer_task` has three input parameters:\n",
    "    1. The output of `product_task`. Since this step only produces one output, we can reference it via `product_task.output`.\n",
    "    2. The emoji output of the `emoji_task` step. See the `emoji` component defined above where we named the output parameters.\n",
    "    3. Similarly, the `emoji_text` named `Output` from the `emoji` component. In case our pipeline passed text that doesn't correspond with an emoji, it'll use this text to construct a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1297857-2dcb-4002-bcfd-357929e9a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"hello-world\",\n",
    "    description=\"An intro pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "\n",
    "# You can change the `text` and `emoji_str` parameters here to update the pipeline output\n",
    "def intro_pipeline(text: str = \"Vertex Pipelines\", emoji_str: str = \"sparkles\"):\n",
    "    product_task = product_name(text)\n",
    "    emoji_task = emoji(emoji_str)\n",
    "    consumer_task = build_sentence(\n",
    "        product_task.output,\n",
    "        emoji_task.outputs[\"emoji\"],\n",
    "        emoji_task.outputs[\"emoji_text\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d7dab-f401-496d-818f-b75d8bf67e4e",
   "metadata": {},
   "source": [
    "## Step 4: Compile & Run The Pipeline\n",
    "\n",
    "With our pipeline defined, we're ready to compile it. The following will generate a `JSON` file that we'll use to run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9aaafc-0381-44a6-82a0-22720db34b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1281: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=intro_pipeline, package_path=\"intro_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3852fc36-9b09-4554-8ee1-a30c380acd1a",
   "metadata": {},
   "source": [
    "Next, instantiate an API client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e03c99d-bbd3-494f-8ff2-7fb2c9ea4347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b839919-54a9-4132-a64a-781e0df0439c",
   "metadata": {},
   "source": [
    "Finally, run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0127d9e3-6849-473a-8844-f7a46b31cfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/hello-world-20220707051442?project=qwiklabs-gcp-01-33949882a483\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"intro_pipeline_job.json\",\n",
    "    # pipeline_root=PIPELINE_ROOT             # this argument is necessary if you did not specify PIPELINE_ROOT as part of the pipeline definition.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafc82d-728b-4547-aa06-48c807f3f3fa",
   "metadata": {},
   "source": [
    "Running the pipeline should generate a link to view the pipeline run in your console. It should look like this when complete:\n",
    "\n",
    "![](https://cdn.qwiklabs.com/k5Jzue01zuwdjgGZFEe3OCIcoFjihCpuheBlT2hE2RI%3D)\n",
    "\n",
    "This pipeline will take 5-6 minutes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f619b3f-1c50-45a7-af82-2ba2a4309b2f",
   "metadata": {},
   "source": [
    "# Creating An End-To-End ML Pipeline\n",
    "\n",
    "It's time to build our first ML pipeline. In this pipeline, we'll use the UCI Machine Learning Dry Beans dataset, from: KOKLU, M. and OZKAN, I.A., (2020), \"Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.\" In Computers and Electronics in Agriculture, 174, 105507. DOI.\n",
    "\n",
    "> **Note**<br>This pipeline will take over 2 hours to complete. So we will not need to wait the entire duration of the pipeline to complete the lab. Follow the steps until our pipeline job has started.\n",
    "\n",
    "This is a tabular dataset, and in our pipeline we'll use the dataset to train, evaluate, and deploy an AutoML model that classifies beans into one of 7 types based on their characteristics.\n",
    "\n",
    "This pipeline will:\n",
    "\n",
    "* Create a Dataset in Vertex AI.\n",
    "* Train a tabular classification model with AutoML.\n",
    "* Get evaluation metrics on this model.\n",
    "* Based on the evaluation metrics, decide whether to deploy the model using conditional logic in Vertex Pipelines. \n",
    "* Deploy the model to an endpoint using Vertex Prediction.\n",
    "\n",
    "Each of the steps outlined will be a component. Most of the pipeline steps will use pre-built components for Vertex AI services via the `google_cloud_pipeline_components` library we imported earlier in this codelab. In this section, we'll define one custom component first. Then, we'll define the rest of the pipeline steps using pre-built components. Pre-built components make it easier to access Vertex AI services, like model training and deployment.\n",
    "\n",
    "The majority of time for this step is for the AutoML training piece of this pipeline, which will take **about an hour**.\n",
    "\n",
    "## Step 1: A Custom Component For Model Evaluation\n",
    "\n",
    "The custom component we'll define will be used towards the end of the pipeline once model training has completed. This component will do a few things:\n",
    "\n",
    "* Get the evaluation metrics from the trained AutoML classification model.\n",
    "* Parse the metrics and render them in the Vertex Pipelines UI.\n",
    "* Compare the metrics to a threshold to determine whether the model should be deployed.\n",
    "\n",
    "Before defining the component, understand its input and output parameters. As input, this pipeline takes some metadata on our Cloud project, the resulting trained model (we'll define this component later), the model's evaluation metrics, and a `thresholds_dict_str`. The `thresholds_dict_str` is something we'll define when you run your pipeline. In the case of this classification model, this will be the area under the ROC curve value for which we should deploy the model. For example, if we pass in 0.95, that means we'd only like our pipeline to deploy the model if this metric is above 95%.\n",
    "\n",
    "The evaluation component returns a string indicating whether or not to deploy the model. Now we will create this custom component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f9276f0-c19c-4768-9362-a2d9a723165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\n",
    "    output_component_file=\"tables_eval_component.yaml\", # Optional: you can use this to load the component later\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    ")\n",
    "def classif_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,  # \"us-central1\",\n",
    "    api_endpoint: str,  # \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str,\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    metricsc: Output[ClassificationMetrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\n",
    "    \"\"\"This function renders evaluation metrics for an AutoML Tabular classification model.\n",
    "    It retrieves the classification model evaluation generated by the AutoML Tabular training\n",
    "    process, does some parsing, and uses that info to render the ROC curve and confusion matrix\n",
    "    for the model. It also uses given metrics threshold information and compares that to the\n",
    "    evaluation results to determine whether the model is sufficiently accurate to deploy.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import logging\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    # Fetch model eval info\n",
    "    def get_eval_info(client, model_name):\n",
    "        \n",
    "        from google.protobuf.json_format import MessageToDict\n",
    "        \n",
    "        response = client.list_model_evaluations(parent=model_name)\n",
    "        metrics_list = []\n",
    "        metrics_string_list = []\n",
    "        \n",
    "        for evaluation in response:\n",
    "            \n",
    "            print(\"model_evaluation\")\n",
    "            print(\" name:\", evaluation.name)\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\n",
    "            \n",
    "            metrics = MessageToDict(evaluation._pb.metrics)\n",
    "            \n",
    "            for metric in metrics.keys(): logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\n",
    "            \n",
    "            metrics_str = json.dumps(metrics)\n",
    "            metrics_list.append(metrics)\n",
    "            metrics_string_list.append(metrics_str)\n",
    "            \n",
    "        return (\n",
    "            evaluation.name,\n",
    "            metrics_list,\n",
    "            metrics_string_list\n",
    "        )\n",
    "    \n",
    "    # Use the given metrics threshold(s) to determine whether the model is accurate enough to deploy.\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\n",
    "        \n",
    "        for k, v in thresholds_dict.items():\n",
    "            \n",
    "            logging.info(\"k {}, v {}\".format(k, v))\n",
    "            \n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # higher is better\n",
    "                if metrics_dict[k] < v:  # if under threshold, don't deploy\n",
    "                    logging.info(\"{} < {}; returning False\".format(metrics_dict[k], v))\n",
    "                    return False\n",
    "                \n",
    "        logging.info(\"threshold checks passed.\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def log_metrics(metrics_list, metricsc):\n",
    "        \n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\n",
    "        \n",
    "        # log the ROC curve\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "        thresholds = []\n",
    "        \n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\n",
    "            \n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\n",
    "            tpr.append(item.get(\"recall\", 0.0))\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\n",
    "            \n",
    "        print(f\"fpr: {fpr}\")\n",
    "        print(f\"tpr: {tpr}\")\n",
    "        print(f\"thresholds: {thresholds}\")\n",
    "        \n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\n",
    "        \n",
    "        # log the confusion matrix\n",
    "        annotations = []\n",
    "        \n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]: annotations.append(item[\"displayName\"])\n",
    "        \n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\n",
    "        metricsc.log_confusion_matrix(\n",
    "            annotations,\n",
    "            test_confusion_matrix[\"rows\"],\n",
    "        )\n",
    "        \n",
    "        # log textual metrics info as well\n",
    "        for metric in metrics_list[0].keys():\n",
    "            \n",
    "            if metric != \"confidenceMetrics\":\n",
    "                \n",
    "                val_string = json.dumps(metrics_list[0][metric])\n",
    "                metrics.log_metric(metric, val_string)\n",
    "                \n",
    "        # metrics.metadata[\"model_type\"] = \"AutoML Tabular classification\"\n",
    "        \n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    aiplatform.init(project=project)\n",
    "    \n",
    "    # extract the model resource name from the input Model Artifact\n",
    "    model_resource_path = model.uri.replace(\"aiplatform://v1/\", \"\")\n",
    "    logging.info(\"model path: %s\", model_resource_path)\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    \n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(client, model_resource_path\n",
    "                                                             )\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\n",
    "    log_metrics(metrics_list, metricsc)\n",
    "    \n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\n",
    "    dep_decision = \"true\" if deploy else \"false\"\n",
    "    \n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\n",
    "    \n",
    "    return (dep_decision,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa5d52-6dd1-48bc-87f4-62200c81cfc6",
   "metadata": {},
   "source": [
    "## Step 2: Adding Google Cloud Pre-Built Components\n",
    "\n",
    "In this step we'll define the rest of our pipeline components and see how they all fit together.\n",
    "\n",
    "First, define the display name for our pipeline run using a timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f997b5a1-135c-4cfe-96af-72ea1a1c531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automl-beans1657172479\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "DISPLAY_NAME = 'automl-beans{}'.format(str(int(time.time())))\n",
    "print(DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d26f2c-2492-496d-890d-50633204919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"automl-tab-beans-training-v2\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bq_source: str = \"bq://aju-dev-demos.beans.beans1\",\n",
    "    display_name: str = DISPLAY_NAME,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "    thresholds_dict_str: str = '{\"auRoc\": 0.95}',\n",
    "):\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(project=project, display_name=display_name, bq_source=bq_source)\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        column_transformations=[\n",
    "            {\"numeric\": {\"column_name\": \"Area\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Perimeter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MajorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"MinorAxisLength\"}},\n",
    "            {\"numeric\": {\"column_name\": \"AspectRation\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Eccentricity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ConvexArea\"}},\n",
    "            {\"numeric\": {\"column_name\": \"EquivDiameter\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Extent\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Solidity\"}},\n",
    "            {\"numeric\": {\"column_name\": \"roundness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"Compactness\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor1\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor2\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor3\"}},\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor4\"}},\n",
    "            {\"categorical\": {\"column_name\": \"Class\"}}\n",
    "        ],\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"Class\"\n",
    "    )\n",
    "    model_eval_task = classif_model_eval_metrics(\n",
    "        project,\n",
    "        gcp_region,\n",
    "        api_endpoint,\n",
    "        thresholds_dict_str,\n",
    "        training_op.outputs[\"model\"]\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(model_eval_task.outputs[\"dep_decision\"] == \"true\", name=\"deploy_decision\"):\n",
    "        \n",
    "        deploy_op = gcc_aip.ModelDeployOp(      # noqa: F841\n",
    "            model=training_op.outputs[\"model\"],\n",
    "            project=project,\n",
    "            machine_type=\"n1-standard-4\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92013ea6-2c72-4ec7-8fc7-653d52a0f871",
   "metadata": {},
   "source": [
    "What's happening in this code:\n",
    "\n",
    "* First, just as in the previous pipeline, we define the input parameters this pipeline takes. We need to set these manually since they don't depend on the output of other steps in the pipeline.\n",
    "* The rest of the pipeline uses a few pre-built components for interacting with Vertex AI services:|\n",
    "    * `TabularDatasetCreateOp` creates a tabular dataset in Vertex AI given a dataset source either in Cloud Storage or BigQuery. \n",
    "        * In this pipeline, we're passing the data via a BigQuery table URL.\n",
    "    * `AutoMLTabularTrainingJobRunOp` kicks off an AutoML training job for a tabular dataset. \n",
    "        * We pass a few configuration parameters to this component, including the model type (in this case, classification), some data on the columns, how long we'd like to run training for, and a pointer to the dataset. \n",
    "        * Notice that to pass in the dataset to this component, we're providing the output of the previous component via `dataset_create_op.outputs[\"dataset\"]`.\n",
    "    * `ModelDeployOp` deploys a given model to an endpoint in Vertex AI. \n",
    "        * There are additional configuration options available, but here we're providing the endpoint machine type, project, and model we'd like to deploy. \n",
    "        * We're passing in the model by accessing the outputs of the training step in your pipeline.\n",
    "* This pipeline also makes use of conditional logic, a feature of Vertex Pipelines that lets us define a condition, along with different branches based on the result of that condition. \n",
    "    * Remember that when we defined the pipeline we passed a `thresholds_dict_str` parameter. \n",
    "        * This is the accuracy threshold we're using to determine whether to deploy our model to an endpoint. \n",
    "    * To implement this, make use of the `Condition` class from the KFP SDK. \n",
    "        * The condition passed in is the output of the custom eval component you defined earlier in this lab. \n",
    "            * If this condition is true, the pipeline will continue to execute the `deploy_op component`. \n",
    "            * If accuracy doesn't meet the predefined threshold, the pipeline will stop here and won't deploy a model.\n",
    "\n",
    "## Step 3: Compile & Run The End-To-End ML Pipeline\n",
    "\n",
    "With the full pipeline defined, it's time to compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ba33b5e-9654-4f0d-bcef-1c88bd673445",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=\"tab_classif_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6a955-a58b-4ddd-957d-7ea965f700ab",
   "metadata": {},
   "source": [
    "Next, kick off a pipeline run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "549a8c79-5fc2-4bc9-b008-639a85f479d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/automl-tab-beans-training-v2-20220707054738?project=qwiklabs-gcp-01-33949882a483\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"tab_classif_pipeline.json\", \n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"display_name\": DISPLAY_NAME\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0e9c7-45e1-49ea-858c-cafca6121eaf",
   "metadata": {},
   "source": [
    "Click on the link shown after running the cell above to see your pipeline in the console. This pipeline will take a little over an hour to run. Most of the time is spent in the AutoML training step. The completed pipeline will look something like this:\n",
    "\n",
    "![](https://cdn.qwiklabs.com/Y%2BSqksizjHLX3Z84cImy0UwltxfTFS%2BNdDJO2OFWnNc%3D)\n",
    "\n",
    "## Step 4: Comparing Metrics Across Pipeline Runs\n",
    "\n",
    "If we run this pipeline multiple times, we may want to compare metrics across runs. We can use the `aiplatform.get_pipeline_df()` method to access run metadata. Here, we'll get metadata for all runs of this pipeline and load it into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fba6cc-07d2-430e-a222-2ac2d9a6d1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
